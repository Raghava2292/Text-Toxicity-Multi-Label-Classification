{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9c73028",
   "metadata": {},
   "source": [
    "# Model building using Word2Vec Vectorization\n",
    "\n",
    "In this file we will try different machine learning models that will trained on the vectorized text data from Word2Vec vectorization.\n",
    "\n",
    "For the multi-label classification we need the **scikit-multilearn** library. There are different approaches to solve this problem.\n",
    "\n",
    "1. Problem transformation - In this method we will transform the multi-label problem into a single-label problem. There are 3 techiques that does this which are:\n",
    "    1. Binary relevance - In this technique it will treat each label as a separate single-label classification problem.\n",
    "    2. Classifier chains - Another technique that does what binary relevance does but additionally preserves the relationship between each target label.\n",
    "    3. Label powerset - This converts the multi-label problem into a multi-class problem by assigning each unique combination of labels into a class. This will preserve the correlation between the features. This is the best technique among the three.\n",
    "\n",
    "2. Ensembles - Custom stacking of base multi-label classifiers. We can try different combinations of individual multi-label classifiers and stack them together to get results. \n",
    "\n",
    "3. Adaptation techniques - These are single-label classification techniques that are improvised to perform multi-label classification. Like the Multi-Label KNN classifier.\n",
    "\n",
    "4. Neural Networks - The neural networks can be used to solve this multi-label problem. Also, we can use the LSTM models which is a modified version of the regular RNN model.\n",
    "\n",
    "5. BERT - Bidirectional Encoder Representations from Transformers. This uses the transformer technology to learn the text data. This performs much better than the above mentioned models and is mostly used in generative AI problems.\n",
    "\n",
    "For this multi-label classfication problem there are two metrics that should be considered. These are **the accuracy and the hamming loss**. These two parameters play a vital role in these problems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f41c0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-multilearn\n",
      "  Downloading scikit_multilearn-0.2.0-py3-none-any.whl (89 kB)\n",
      "     ---------------------------------------- 0.0/89.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 89.4/89.4 kB 4.9 MB/s eta 0:00:00\n",
      "Installing collected packages: scikit-multilearn\n",
      "Successfully installed scikit-multilearn-0.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: textract 1.6.5 has a non-standard dependency specifier extract-msg<=0.29.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of textract or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-multilearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645cc15f",
   "metadata": {},
   "source": [
    "## Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8ca7ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import string\n",
    "\n",
    "#nltk-preprocessing\n",
    "import nltk\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "#plotting\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#misc\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tqdm.notebook import tqdm\n",
    "from itertools import combinations\n",
    "\n",
    "#multi-processing\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool,freeze_support\n",
    "from multiprocessing import Process\n",
    "\n",
    "#multi-label \n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "\n",
    "#metrics\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import roc_curve, auc,roc_auc_score\n",
    "\n",
    "#modelling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#Tensor flow for MLP\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense,Input,Activation,Dropout,BatchNormalization\n",
    "from tensorflow.keras.models import Model,Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "#model loading\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92a5cdc",
   "metadata": {},
   "source": [
    "## Loading the w2v encoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faea65dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.051367</td>\n",
       "      <td>-0.060495</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>-0.049499</td>\n",
       "      <td>0.029956</td>\n",
       "      <td>-0.145452</td>\n",
       "      <td>0.086651</td>\n",
       "      <td>-0.000229</td>\n",
       "      <td>0.064938</td>\n",
       "      <td>-0.536234</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015343</td>\n",
       "      <td>-0.158155</td>\n",
       "      <td>-0.082105</td>\n",
       "      <td>0.091479</td>\n",
       "      <td>0.108467</td>\n",
       "      <td>0.030422</td>\n",
       "      <td>-0.005188</td>\n",
       "      <td>-0.003624</td>\n",
       "      <td>0.038748</td>\n",
       "      <td>0.181494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.077654</td>\n",
       "      <td>0.083294</td>\n",
       "      <td>0.137685</td>\n",
       "      <td>0.078101</td>\n",
       "      <td>0.008173</td>\n",
       "      <td>-0.049521</td>\n",
       "      <td>0.008864</td>\n",
       "      <td>-0.071333</td>\n",
       "      <td>0.035709</td>\n",
       "      <td>-0.788647</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028346</td>\n",
       "      <td>0.098004</td>\n",
       "      <td>-0.125618</td>\n",
       "      <td>0.067190</td>\n",
       "      <td>0.184657</td>\n",
       "      <td>0.127138</td>\n",
       "      <td>0.205400</td>\n",
       "      <td>0.043030</td>\n",
       "      <td>0.080688</td>\n",
       "      <td>-0.006079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.284304</td>\n",
       "      <td>0.001876</td>\n",
       "      <td>0.023268</td>\n",
       "      <td>0.028583</td>\n",
       "      <td>0.042766</td>\n",
       "      <td>0.063144</td>\n",
       "      <td>-0.146772</td>\n",
       "      <td>0.056002</td>\n",
       "      <td>0.172073</td>\n",
       "      <td>-1.142111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098520</td>\n",
       "      <td>-0.078259</td>\n",
       "      <td>-0.090264</td>\n",
       "      <td>-0.095416</td>\n",
       "      <td>-0.019609</td>\n",
       "      <td>-0.153735</td>\n",
       "      <td>-0.002546</td>\n",
       "      <td>0.070028</td>\n",
       "      <td>0.077496</td>\n",
       "      <td>0.125922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.221880</td>\n",
       "      <td>0.030425</td>\n",
       "      <td>0.071590</td>\n",
       "      <td>-0.156064</td>\n",
       "      <td>-0.031576</td>\n",
       "      <td>0.028836</td>\n",
       "      <td>-0.090924</td>\n",
       "      <td>0.163952</td>\n",
       "      <td>-0.048329</td>\n",
       "      <td>-1.188217</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054491</td>\n",
       "      <td>-0.033830</td>\n",
       "      <td>-0.026168</td>\n",
       "      <td>0.061329</td>\n",
       "      <td>0.075770</td>\n",
       "      <td>0.200697</td>\n",
       "      <td>0.002465</td>\n",
       "      <td>-0.116672</td>\n",
       "      <td>0.059649</td>\n",
       "      <td>-0.067360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.041364</td>\n",
       "      <td>-0.055539</td>\n",
       "      <td>0.264545</td>\n",
       "      <td>-0.351085</td>\n",
       "      <td>0.183440</td>\n",
       "      <td>0.052547</td>\n",
       "      <td>-0.114125</td>\n",
       "      <td>-0.145492</td>\n",
       "      <td>-0.153485</td>\n",
       "      <td>-0.621002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048379</td>\n",
       "      <td>0.120352</td>\n",
       "      <td>0.014540</td>\n",
       "      <td>-0.138365</td>\n",
       "      <td>-0.111523</td>\n",
       "      <td>-0.004556</td>\n",
       "      <td>0.024031</td>\n",
       "      <td>-0.004104</td>\n",
       "      <td>-0.278342</td>\n",
       "      <td>0.194327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158738</th>\n",
       "      <td>0.024454</td>\n",
       "      <td>0.046005</td>\n",
       "      <td>-0.113831</td>\n",
       "      <td>-0.132215</td>\n",
       "      <td>0.008551</td>\n",
       "      <td>0.063240</td>\n",
       "      <td>-0.024901</td>\n",
       "      <td>0.105503</td>\n",
       "      <td>-0.006373</td>\n",
       "      <td>-1.530968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.067110</td>\n",
       "      <td>0.006946</td>\n",
       "      <td>0.118147</td>\n",
       "      <td>0.158259</td>\n",
       "      <td>0.110933</td>\n",
       "      <td>0.028245</td>\n",
       "      <td>-0.085713</td>\n",
       "      <td>-0.095958</td>\n",
       "      <td>0.007209</td>\n",
       "      <td>-0.056857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158739</th>\n",
       "      <td>0.334859</td>\n",
       "      <td>0.012415</td>\n",
       "      <td>0.153593</td>\n",
       "      <td>-0.124930</td>\n",
       "      <td>-0.073279</td>\n",
       "      <td>0.034587</td>\n",
       "      <td>-0.089429</td>\n",
       "      <td>-0.016419</td>\n",
       "      <td>0.249144</td>\n",
       "      <td>-0.831725</td>\n",
       "      <td>...</td>\n",
       "      <td>0.267108</td>\n",
       "      <td>-0.331805</td>\n",
       "      <td>-0.096106</td>\n",
       "      <td>0.000979</td>\n",
       "      <td>0.122380</td>\n",
       "      <td>-0.629693</td>\n",
       "      <td>0.056388</td>\n",
       "      <td>0.370078</td>\n",
       "      <td>-0.086796</td>\n",
       "      <td>0.377776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158740</th>\n",
       "      <td>0.132612</td>\n",
       "      <td>0.093696</td>\n",
       "      <td>0.039911</td>\n",
       "      <td>-0.208920</td>\n",
       "      <td>-0.156506</td>\n",
       "      <td>0.039680</td>\n",
       "      <td>0.092735</td>\n",
       "      <td>-0.097160</td>\n",
       "      <td>-0.355542</td>\n",
       "      <td>-0.799413</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.159605</td>\n",
       "      <td>-0.004408</td>\n",
       "      <td>-0.093319</td>\n",
       "      <td>0.113729</td>\n",
       "      <td>0.163493</td>\n",
       "      <td>0.052583</td>\n",
       "      <td>0.125530</td>\n",
       "      <td>0.188881</td>\n",
       "      <td>-0.200602</td>\n",
       "      <td>-0.208021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158741</th>\n",
       "      <td>-0.226283</td>\n",
       "      <td>-0.112413</td>\n",
       "      <td>-0.169718</td>\n",
       "      <td>-0.211149</td>\n",
       "      <td>0.049545</td>\n",
       "      <td>0.164669</td>\n",
       "      <td>-0.129212</td>\n",
       "      <td>-0.046732</td>\n",
       "      <td>0.092141</td>\n",
       "      <td>-1.198321</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068701</td>\n",
       "      <td>-0.116216</td>\n",
       "      <td>-0.040594</td>\n",
       "      <td>0.025550</td>\n",
       "      <td>-0.071042</td>\n",
       "      <td>-0.289346</td>\n",
       "      <td>0.053791</td>\n",
       "      <td>-0.165067</td>\n",
       "      <td>0.111406</td>\n",
       "      <td>0.173672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158742</th>\n",
       "      <td>-0.018888</td>\n",
       "      <td>-0.121526</td>\n",
       "      <td>-0.181386</td>\n",
       "      <td>0.030481</td>\n",
       "      <td>0.043399</td>\n",
       "      <td>-0.176498</td>\n",
       "      <td>0.022072</td>\n",
       "      <td>0.167469</td>\n",
       "      <td>0.230825</td>\n",
       "      <td>-1.539024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028942</td>\n",
       "      <td>-0.195147</td>\n",
       "      <td>0.006169</td>\n",
       "      <td>0.056025</td>\n",
       "      <td>0.167023</td>\n",
       "      <td>-0.230338</td>\n",
       "      <td>0.017020</td>\n",
       "      <td>-0.200753</td>\n",
       "      <td>0.038282</td>\n",
       "      <td>0.224683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>158743 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2         3         4         5         6  \\\n",
       "0      -0.051367 -0.060495  0.001149 -0.049499  0.029956 -0.145452  0.086651   \n",
       "1       0.077654  0.083294  0.137685  0.078101  0.008173 -0.049521  0.008864   \n",
       "2      -0.284304  0.001876  0.023268  0.028583  0.042766  0.063144 -0.146772   \n",
       "3      -0.221880  0.030425  0.071590 -0.156064 -0.031576  0.028836 -0.090924   \n",
       "4       0.041364 -0.055539  0.264545 -0.351085  0.183440  0.052547 -0.114125   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "158738  0.024454  0.046005 -0.113831 -0.132215  0.008551  0.063240 -0.024901   \n",
       "158739  0.334859  0.012415  0.153593 -0.124930 -0.073279  0.034587 -0.089429   \n",
       "158740  0.132612  0.093696  0.039911 -0.208920 -0.156506  0.039680  0.092735   \n",
       "158741 -0.226283 -0.112413 -0.169718 -0.211149  0.049545  0.164669 -0.129212   \n",
       "158742 -0.018888 -0.121526 -0.181386  0.030481  0.043399 -0.176498  0.022072   \n",
       "\n",
       "               7         8         9  ...       290       291       292  \\\n",
       "0      -0.000229  0.064938 -0.536234  ... -0.015343 -0.158155 -0.082105   \n",
       "1      -0.071333  0.035709 -0.788647  ... -0.028346  0.098004 -0.125618   \n",
       "2       0.056002  0.172073 -1.142111  ...  0.098520 -0.078259 -0.090264   \n",
       "3       0.163952 -0.048329 -1.188217  ...  0.054491 -0.033830 -0.026168   \n",
       "4      -0.145492 -0.153485 -0.621002  ...  0.048379  0.120352  0.014540   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "158738  0.105503 -0.006373 -1.530968  ... -0.067110  0.006946  0.118147   \n",
       "158739 -0.016419  0.249144 -0.831725  ...  0.267108 -0.331805 -0.096106   \n",
       "158740 -0.097160 -0.355542 -0.799413  ... -0.159605 -0.004408 -0.093319   \n",
       "158741 -0.046732  0.092141 -1.198321  ...  0.068701 -0.116216 -0.040594   \n",
       "158742  0.167469  0.230825 -1.539024  ... -0.028942 -0.195147  0.006169   \n",
       "\n",
       "             293       294       295       296       297       298       299  \n",
       "0       0.091479  0.108467  0.030422 -0.005188 -0.003624  0.038748  0.181494  \n",
       "1       0.067190  0.184657  0.127138  0.205400  0.043030  0.080688 -0.006079  \n",
       "2      -0.095416 -0.019609 -0.153735 -0.002546  0.070028  0.077496  0.125922  \n",
       "3       0.061329  0.075770  0.200697  0.002465 -0.116672  0.059649 -0.067360  \n",
       "4      -0.138365 -0.111523 -0.004556  0.024031 -0.004104 -0.278342  0.194327  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "158738  0.158259  0.110933  0.028245 -0.085713 -0.095958  0.007209 -0.056857  \n",
       "158739  0.000979  0.122380 -0.629693  0.056388  0.370078 -0.086796  0.377776  \n",
       "158740  0.113729  0.163493  0.052583  0.125530  0.188881 -0.200602 -0.208021  \n",
       "158741  0.025550 -0.071042 -0.289346  0.053791 -0.165067  0.111406  0.173672  \n",
       "158742  0.056025  0.167023 -0.230338  0.017020 -0.200753  0.038282  0.224683  \n",
       "\n",
       "[158743 rows x 300 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pd.read_csv('encoded_text_w2v.csv', index_col = 0)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cff2223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Toxic</th>\n",
       "      <th>Severe Toxic</th>\n",
       "      <th>Obscene</th>\n",
       "      <th>Threat</th>\n",
       "      <th>Insult</th>\n",
       "      <th>Hatred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166000</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166001</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166002</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166003</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>158743 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Toxic  Severe Toxic  Obscene  Threat  Insult  Hatred\n",
       "0           0             0        0       0       0       0\n",
       "1           0             0        0       0       0       0\n",
       "2           0             0        0       0       0       0\n",
       "3           0             0        0       0       0       0\n",
       "4           0             0        0       0       0       0\n",
       "...       ...           ...      ...     ...     ...     ...\n",
       "165999      0             0        0       0       0       0\n",
       "166000      0             0        0       0       0       0\n",
       "166001      0             0        0       0       0       0\n",
       "166002      0             0        0       0       0       0\n",
       "166003      0             0        0       0       0       0\n",
       "\n",
       "[158743 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.read_csv('targets.csv', index_col = 0)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37e452c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "401823ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126994, 300)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f72f1627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31749, 300)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0f0064",
   "metadata": {},
   "source": [
    "## Model building\n",
    "\n",
    "### 1. Problem transformation\n",
    "### 1.1 Binary relevance\n",
    "\n",
    "Here a list of classifiers will be prepared and all of them will be trained and evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db6b70e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25a9ee1075004172b60a78f28de71929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Exact Match Ratio (Accuracy)</th>\n",
       "      <th>Average AUC</th>\n",
       "      <th>Hamming-Loss</th>\n",
       "      <th>Log-Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gaussian NB</td>\n",
       "      <td>0.764811</td>\n",
       "      <td>0.886629</td>\n",
       "      <td>0.115410</td>\n",
       "      <td>0.693718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.901887</td>\n",
       "      <td>0.950726</td>\n",
       "      <td>0.025366</td>\n",
       "      <td>1.564307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.797883</td>\n",
       "      <td>0.648733</td>\n",
       "      <td>0.048290</td>\n",
       "      <td>2.403681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.904406</td>\n",
       "      <td>0.897222</td>\n",
       "      <td>0.027245</td>\n",
       "      <td>1.503654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.905698</td>\n",
       "      <td>0.953565</td>\n",
       "      <td>0.023450</td>\n",
       "      <td>1.594186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Classifier  Exact Match Ratio (Accuracy)  Average AUC  \\\n",
       "0          Gaussian NB                      0.764811     0.886629   \n",
       "1  Logistic Regression                      0.901887     0.950726   \n",
       "2        Decision Tree                      0.797883     0.648733   \n",
       "3        Random Forest                      0.904406     0.897222   \n",
       "4              XGBoost                      0.905698     0.953565   \n",
       "\n",
       "   Hamming-Loss  Log-Loss  \n",
       "0      0.115410  0.693718  \n",
       "1      0.025366  1.564307  \n",
       "2      0.048290  2.403681  \n",
       "3      0.027245  1.503654  \n",
       "4      0.023450  1.594186  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_list=[GaussianNB(),LogisticRegression(),\n",
    "          DecisionTreeClassifier(),RandomForestClassifier(),XGBClassifier()]\n",
    "\n",
    "acc=[]\n",
    "ham_loss=[]\n",
    "logloss=[]\n",
    "avg_auc=[]\n",
    "\n",
    "for base_clf in tqdm(clf_list):\n",
    "    clf = BinaryRelevance(base_clf)\n",
    "    clf.fit(x_train, y_train)\n",
    "    pred = clf.predict(x_test)\n",
    "    pred_proba=clf.predict_proba(x_test)\n",
    "    acc.append(accuracy_score(y_test,pred))\n",
    "    avg_auc.append(np.mean(roc_auc_score(y_test, pred_proba.A, average=None)))\n",
    "    ham_loss.append(hamming_loss(y_test,pred))\n",
    "    logloss.append(log_loss(y_test,pred.A))\n",
    "\n",
    "bin_rel_res=pd.DataFrame(columns=['Classifier','Exact Match Ratio (Accuracy)','Average AUC',\n",
    "                                  'Hamming-Loss','Log-Loss'])\n",
    "bin_rel_res['Classifier']=['Gaussian NB','Logistic Regression','Decision Tree','Random Forest','XGBoost']\n",
    "bin_rel_res['Exact Match Ratio (Accuracy)']=acc\n",
    "bin_rel_res['Hamming-Loss']=ham_loss\n",
    "bin_rel_res['Log-Loss']=logloss\n",
    "bin_rel_res['Average AUC']=avg_auc\n",
    "bin_rel_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e1f081",
   "metadata": {},
   "source": [
    "----------\n",
    "\n",
    "From the above results we can see that the Logistic Regression, Random Forest, and the XGBoost models are performing much better than the others. So, for the other techniques it will be best to use only these 3 models as this is highly computationally expensive.\n",
    "\n",
    "### 1.2 Classifier chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5fb575d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd48f8c1b7f943a0b16d6427d638a41b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Exact Match Ratio (Accuracy)</th>\n",
       "      <th>Average AUC</th>\n",
       "      <th>Hamming-Loss</th>\n",
       "      <th>Log-Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.90491</td>\n",
       "      <td>0.932560</td>\n",
       "      <td>0.025334</td>\n",
       "      <td>1.169596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.90721</td>\n",
       "      <td>0.880164</td>\n",
       "      <td>0.026227</td>\n",
       "      <td>1.196832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.90869</td>\n",
       "      <td>0.942104</td>\n",
       "      <td>0.023486</td>\n",
       "      <td>1.349585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Classifier  Exact Match Ratio (Accuracy)  Average AUC  \\\n",
       "0  Logistic Regression                       0.90491     0.932560   \n",
       "1        Random Forest                       0.90721     0.880164   \n",
       "2              XGBoost                       0.90869     0.942104   \n",
       "\n",
       "   Hamming-Loss  Log-Loss  \n",
       "0      0.025334  1.169596  \n",
       "1      0.026227  1.196832  \n",
       "2      0.023486  1.349585  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_list=[LogisticRegression(),RandomForestClassifier(),XGBClassifier()]\n",
    "\n",
    "acc=[]\n",
    "ham_loss=[]\n",
    "logloss=[]\n",
    "avg_auc=[]\n",
    "\n",
    "for base_clf in tqdm(clf_list):\n",
    "    clf = ClassifierChain(base_clf)\n",
    "    clf.fit(x_train, y_train)\n",
    "    pred = clf.predict(x_test)\n",
    "    pred_proba=clf.predict_proba(x_test)\n",
    "    acc.append(accuracy_score(y_test,pred))\n",
    "    avg_auc.append(np.mean(roc_auc_score(y_test, pred_proba.A, average=None)))\n",
    "    ham_loss.append(hamming_loss(y_test,pred))\n",
    "    logloss.append(log_loss(y_test,pred.A))\n",
    "\n",
    "clf_chain_res=pd.DataFrame(columns=['Classifier','Exact Match Ratio (Accuracy)',\n",
    "                                  'Average AUC','Hamming-Loss','Log-Loss'])\n",
    "clf_chain_res['Classifier']=['Logistic Regression','Random Forest','XGBoost']\n",
    "clf_chain_res['Exact Match Ratio (Accuracy)']=acc\n",
    "clf_chain_res['Hamming-Loss']=ham_loss\n",
    "clf_chain_res['Log-Loss']=logloss\n",
    "clf_chain_res['Average AUC']=avg_auc\n",
    "clf_chain_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be706b7a",
   "metadata": {},
   "source": [
    "---------\n",
    "\n",
    "There is a negligible improvement in the results. Among the 3 the XGBoost is giving the best results.\n",
    "\n",
    "### 1.3 Label Powerset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a896d58e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb11da81621f4552afbce04a19b98fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Exact Match Ratio (Accuracy)</th>\n",
       "      <th>Average AUC</th>\n",
       "      <th>Hamming-Loss</th>\n",
       "      <th>Log-Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.905855</td>\n",
       "      <td>0.952273</td>\n",
       "      <td>0.025838</td>\n",
       "      <td>1.123778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.902895</td>\n",
       "      <td>0.906882</td>\n",
       "      <td>0.030190</td>\n",
       "      <td>0.741122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.907367</td>\n",
       "      <td>0.953201</td>\n",
       "      <td>0.024961</td>\n",
       "      <td>1.336620</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Classifier  Exact Match Ratio (Accuracy)  Average AUC  \\\n",
       "0  Logistic Regression                      0.905855     0.952273   \n",
       "1        Random Forest                      0.902895     0.906882   \n",
       "2              XGBoost                      0.907367     0.953201   \n",
       "\n",
       "   Hamming-Loss  Log-Loss  \n",
       "0      0.025838  1.123778  \n",
       "1      0.030190  0.741122  \n",
       "2      0.024961  1.336620  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_list=[LogisticRegression(),RandomForestClassifier(),XGBClassifier()]\n",
    "\n",
    "acc=[]\n",
    "ham_loss=[]\n",
    "logloss=[]\n",
    "avg_auc=[]\n",
    "\n",
    "for base_clf in tqdm(clf_list):\n",
    "    clf = LabelPowerset(base_clf)\n",
    "    clf.fit(x_train, y_train)\n",
    "    pred = clf.predict(x_test)\n",
    "    pred_proba=clf.predict_proba(x_test)\n",
    "    acc.append(accuracy_score(y_test,pred))\n",
    "    avg_auc.append(np.mean(roc_auc_score(y_test, pred_proba.A, average=None)))\n",
    "    ham_loss.append(hamming_loss(y_test,pred))\n",
    "    logloss.append(log_loss(y_test,pred.A))\n",
    "\n",
    "lbl_pwr_set_res=pd.DataFrame(columns=['Classifier','Exact Match Ratio (Accuracy)',\n",
    "                                  'Average AUC','Hamming-Loss','Log-Loss'])\n",
    "lbl_pwr_set_res['Classifier']=['Logistic Regression','Random Forest','XGBoost']\n",
    "lbl_pwr_set_res['Exact Match Ratio (Accuracy)']=acc\n",
    "lbl_pwr_set_res['Hamming-Loss']=ham_loss\n",
    "lbl_pwr_set_res['Log-Loss']=logloss\n",
    "lbl_pwr_set_res['Average AUC']=avg_auc\n",
    "lbl_pwr_set_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942a1f2a",
   "metadata": {},
   "source": [
    "### 2. Ensemble methods\n",
    "These methods are an aggregation of individual models. Either the same model is grouped together multiple times and different models can be grouped together. Different models grouped together is called stacking of the models.\n",
    "\n",
    "The main disadvantage of these ensemble methods is that the improvement in the performance is not guaranteed but it will definitely take a lot of time to train the models. So, it is better not to use them unless highly required. So, in this project these ensemble methods are not used as there are better models like LSTM and BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1597a75",
   "metadata": {},
   "source": [
    "### 3. Adopted algorithm\n",
    "\n",
    "### 3.1 MLKNN\n",
    "The regular KNN model modified to work for the multi-label classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "755c93be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLkNN(k=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLkNN</label><div class=\"sk-toggleable__content\"><pre>MLkNN(k=3)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLkNN(k=3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skmultilearn.adapt import MLkNN\n",
    "mlknn = MLkNN(k=3)\n",
    "mlknn.fit(np.array(x_train), np.array(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00033baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_score(Y_test,predict): \n",
    "    loss = hamming_loss(Y_test,predict)\n",
    "    print(\"Hamming_loss : {}\".format(loss))\n",
    "    accuracy = accuracy_score(Y_test,predict)\n",
    "    print(\"Accuracy : {}\".format(accuracy*100))\n",
    "    try : \n",
    "        loss = log_loss(Y_test,predict)\n",
    "    except :\n",
    "        loss = log_loss(Y_test,predict.toarray())\n",
    "    print(\"Log_loss : {}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d12b3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pred = mlknn.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6b5319d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming_loss : 0.028300103940281585\n",
      "Accuracy : 89.34139657941984\n",
      "Log_loss : 1.3628663857268795\n"
     ]
    }
   ],
   "source": [
    "evaluate_score(y_test, knn_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2af75fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_acc = {}\n",
    "knn_hamm = {}\n",
    "for i in range(3, 10):\n",
    "    knn = MLkNN(k=i)\n",
    "    knn.fit(np.array(x_train), np.array(y_train))\n",
    "    pred = knn.predict(x_test)\n",
    "    knn_acc[i] = accuracy_score(y_test, pred)\n",
    "    knn_hamm[i] = hamming_loss(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "852b800d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: 0.8934139657941983,\n",
       " 4: 0.8816340672147154,\n",
       " 5: 0.9000283473495229,\n",
       " 6: 0.8946423509401871,\n",
       " 7: 0.9017606853759174,\n",
       " 8: 0.8987054710384579,\n",
       " 9: 0.896059718416328}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "449c3b9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: 0.028300103940281585,\n",
       " 4: 0.029670225833884532,\n",
       " 5: 0.02622654781777904,\n",
       " 6: 0.027155710941027013,\n",
       " 7: 0.025811836593278528,\n",
       " 8: 0.026058563524310477,\n",
       " 9: 0.02597982088674709}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_hamm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb771fc0",
   "metadata": {},
   "source": [
    "-------------\n",
    "\n",
    "From the above results we can see the 7 is the best choice for the k value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1fba36b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLkNN(k=7)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLkNN</label><div class=\"sk-toggleable__content\"><pre>MLkNN(k=7)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLkNN(k=7)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlknn_7 = MLkNN(k=7)\n",
    "mlknn_7.fit(np.array(x_train), np.array(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2913f4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_7_pred = mlknn_7.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0067946e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming_loss : 0.025811836593278528\n",
      "Accuracy : 90.17606853759173\n",
      "Log_loss : 1.327427909080764\n"
     ]
    }
   ],
   "source": [
    "evaluate_score(y_test, knn_7_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecd0595",
   "metadata": {},
   "source": [
    "### 3.2 BRKNN\n",
    "The regular KNN model combined with Binary Relevance technique mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "979307f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BRkNNaClassifier(k=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BRkNNaClassifier</label><div class=\"sk-toggleable__content\"><pre>BRkNNaClassifier(k=3)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "BRkNNaClassifier(k=3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skmultilearn.adapt import BRkNNaClassifier\n",
    "brknn = BRkNNaClassifier(k=3)\n",
    "brknn.fit(np.array(x_train), np.array(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b382ac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "brknn_pred = brknn.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32555514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming_loss : 0.027827648114901255\n",
      "Accuracy : 89.47998362153139\n",
      "Log_loss : 1.3210079633443539\n"
     ]
    }
   ],
   "source": [
    "evaluate_score(y_test, brknn_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70514991",
   "metadata": {},
   "outputs": [],
   "source": [
    "brknn_acc = {}\n",
    "brknn_hamm = {}\n",
    "for i in range(3, 10):\n",
    "    brknn = BRkNNaClassifier(k=i)\n",
    "    brknn.fit(np.array(x_train), np.array(y_train))\n",
    "    pred = brknn.predict(x_test)\n",
    "    brknn_acc[i] = accuracy_score(y_test, pred)\n",
    "    brknn_hamm[i] = hamming_loss(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f2d62c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: 0.8947998362153139,\n",
       " 4: 0.9068632082900249,\n",
       " 5: 0.9023906264764244,\n",
       " 6: 0.9067057230148982,\n",
       " 7: 0.9048158997133768,\n",
       " 8: 0.9081230904910391,\n",
       " 9: 0.9067372200699234}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brknn_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f33f44b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: 0.027827648114901255,\n",
       " 4: 0.02554411162556301,\n",
       " 5: 0.02587483070332924,\n",
       " 6: 0.025176645983600532,\n",
       " 7: 0.02529213518536017,\n",
       " 8: 0.024819679359979842,\n",
       " 9: 0.02487742396085966}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brknn_hamm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb013be",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "From the above results we can see that 8 is the best choice for the k value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc6362cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BRkNNaClassifier(k=8)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BRkNNaClassifier</label><div class=\"sk-toggleable__content\"><pre>BRkNNaClassifier(k=8)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "BRkNNaClassifier(k=8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brknn_8 = BRkNNaClassifier(k=8)\n",
    "brknn_8.fit(np.array(x_train), np.array(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "baeb6372",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming_loss : 0.024819679359979842\n",
      "Accuracy : 90.81230904910392\n",
      "Log_loss : 1.3319683407697038\n"
     ]
    }
   ],
   "source": [
    "brknn_8_pred = brknn_8.predict(x_test)\n",
    "evaluate_score(y_test, brknn_8_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33833a08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126994, 300)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192990df",
   "metadata": {},
   "source": [
    "### 4. Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ecad012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Dropout\n",
    "\n",
    "# Defining the model\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(300,input_dim = 300,kernel_initializer = 'he_uniform',activation = 'relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(150,kernel_initializer = 'he_uniform',activation = 'relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(75,kernel_initializer = 'he_uniform',activation = 'relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(6,activation = 'sigmoid'))\n",
    "    \n",
    "    adam = Adam(lr = 0.01) #sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
    "    model.compile(loss = 'binary_crossentropy',optimizer = adam,metrics = ['binary_accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6cfb52cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape[1]/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a74942f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ann = KerasClassifier(build_fn = create_model,verbose = 0,batch_size = 2048,epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4b892206",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a38ad1b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1985/1985 [==============================] - 9s 4ms/step - loss: 0.0792 - binary_accuracy: 0.9746\n",
      "Epoch 2/50\n",
      "1985/1985 [==============================] - 8s 4ms/step - loss: 0.0738 - binary_accuracy: 0.9760\n",
      "Epoch 3/50\n",
      "1985/1985 [==============================] - 9s 4ms/step - loss: 0.0721 - binary_accuracy: 0.9762\n",
      "Epoch 4/50\n",
      "1985/1985 [==============================] - 9s 4ms/step - loss: 0.0710 - binary_accuracy: 0.9767\n",
      "Epoch 5/50\n",
      "1985/1985 [==============================] - 9s 4ms/step - loss: 0.0696 - binary_accuracy: 0.9767\n",
      "Epoch 6/50\n",
      "1985/1985 [==============================] - 9s 4ms/step - loss: 0.0686 - binary_accuracy: 0.9770\n",
      "Epoch 7/50\n",
      "1985/1985 [==============================] - 9s 5ms/step - loss: 0.0684 - binary_accuracy: 0.9772\n",
      "Epoch 8/50\n",
      "1985/1985 [==============================] - 9s 4ms/step - loss: 0.0670 - binary_accuracy: 0.9773\n",
      "Epoch 9/50\n",
      "1985/1985 [==============================] - 9s 5ms/step - loss: 0.0664 - binary_accuracy: 0.9775\n",
      "Epoch 10/50\n",
      "1985/1985 [==============================] - 9s 4ms/step - loss: 0.0647 - binary_accuracy: 0.9776\n",
      "Epoch 11/50\n",
      "1985/1985 [==============================] - 9s 4ms/step - loss: 0.0661 - binary_accuracy: 0.9775\n",
      "Epoch 12/50\n",
      "1985/1985 [==============================] - 9s 4ms/step - loss: 0.0658 - binary_accuracy: 0.9776\n",
      "Epoch 13/50\n",
      "1985/1985 [==============================] - 9s 5ms/step - loss: 0.0647 - binary_accuracy: 0.9776\n",
      "Epoch 14/50\n",
      "1985/1985 [==============================] - 9s 4ms/step - loss: 0.0650 - binary_accuracy: 0.9775\n",
      "Epoch 15/50\n",
      "1985/1985 [==============================] - 8s 4ms/step - loss: 0.0642 - binary_accuracy: 0.9778\n",
      "Epoch 16/50\n",
      "1985/1985 [==============================] - 9s 4ms/step - loss: 0.0651 - binary_accuracy: 0.9777\n",
      "Epoch 17/50\n",
      "1985/1985 [==============================] - 9s 4ms/step - loss: 0.0632 - binary_accuracy: 0.9780\n",
      "Epoch 18/50\n",
      "1985/1985 [==============================] - 9s 4ms/step - loss: 0.0635 - binary_accuracy: 0.9782\n",
      "Epoch 19/50\n",
      "1985/1985 [==============================] - 9s 4ms/step - loss: 0.0630 - binary_accuracy: 0.9782\n",
      "Epoch 20/50\n",
      "1985/1985 [==============================] - 9s 4ms/step - loss: 0.0619 - binary_accuracy: 0.9783\n",
      "Epoch 21/50\n",
      "1985/1985 [==============================] - 9s 4ms/step - loss: 0.0635 - binary_accuracy: 0.9783\n",
      "Epoch 22/50\n",
      "1985/1985 [==============================] - 9s 4ms/step - loss: 0.0636 - binary_accuracy: 0.9783\n",
      "Epoch 23/50\n",
      "1985/1985 [==============================] - 9s 4ms/step - loss: 0.0668 - binary_accuracy: 0.9779\n",
      "Epoch 24/50\n",
      "1985/1985 [==============================] - 9s 4ms/step - loss: 0.0734 - binary_accuracy: 0.9769\n",
      "Epoch 25/50\n",
      "1985/1985 [==============================] - 9s 4ms/step - loss: 0.0708 - binary_accuracy: 0.9773\n",
      "Epoch 26/50\n",
      "1985/1985 [==============================] - 9s 4ms/step - loss: 0.0701 - binary_accuracy: 0.9774\n",
      "Epoch 27/50\n",
      "1985/1985 [==============================] - 9s 4ms/step - loss: 0.0688 - binary_accuracy: 0.9773\n",
      "Epoch 28/50\n",
      "1985/1985 [==============================] - 9s 4ms/step - loss: 0.0691 - binary_accuracy: 0.9775\n",
      "Epoch 29/50\n",
      "1985/1985 [==============================] - 9s 4ms/step - loss: 0.0692 - binary_accuracy: 0.9773\n",
      "Epoch 30/50\n",
      "1985/1985 [==============================] - 9s 4ms/step - loss: 0.0700 - binary_accuracy: 0.9774\n",
      "Epoch 31/50\n",
      "1985/1985 [==============================] - 9s 4ms/step - loss: 0.0683 - binary_accuracy: 0.9776\n",
      "Epoch 32/50\n",
      "1985/1985 [==============================] - 9s 4ms/step - loss: 0.0684 - binary_accuracy: 0.9775\n",
      "Epoch 33/50\n",
      "1985/1985 [==============================] - 9s 4ms/step - loss: 0.0675 - binary_accuracy: 0.9778\n",
      "Epoch 34/50\n",
      "1985/1985 [==============================] - 9s 4ms/step - loss: 0.0691 - binary_accuracy: 0.9774\n",
      "Epoch 35/50\n",
      "1985/1985 [==============================] - 9s 4ms/step - loss: 0.0680 - binary_accuracy: 0.9775\n",
      "Epoch 36/50\n",
      "1985/1985 [==============================] - 9s 4ms/step - loss: 0.0686 - binary_accuracy: 0.9775\n",
      "Epoch 37/50\n",
      "1985/1985 [==============================] - 9s 4ms/step - loss: 0.0672 - binary_accuracy: 0.9778\n",
      "Epoch 38/50\n",
      "1985/1985 [==============================] - 9s 4ms/step - loss: 0.0684 - binary_accuracy: 0.9780\n",
      "Epoch 39/50\n",
      "1985/1985 [==============================] - 9s 4ms/step - loss: 0.0668 - binary_accuracy: 0.9780\n",
      "Epoch 40/50\n",
      "1985/1985 [==============================] - 9s 4ms/step - loss: 0.0672 - binary_accuracy: 0.9781\n",
      "Epoch 41/50\n",
      "1985/1985 [==============================] - 9s 4ms/step - loss: 0.0659 - binary_accuracy: 0.9784\n",
      "Epoch 42/50\n",
      "1985/1985 [==============================] - 9s 4ms/step - loss: 0.0665 - binary_accuracy: 0.9785\n",
      "Epoch 43/50\n",
      "1985/1985 [==============================] - 9s 4ms/step - loss: 0.0661 - binary_accuracy: 0.9783\n",
      "Epoch 44/50\n",
      "1985/1985 [==============================] - 9s 4ms/step - loss: 0.0659 - binary_accuracy: 0.9783\n",
      "Epoch 45/50\n",
      "1985/1985 [==============================] - 9s 4ms/step - loss: 0.0652 - binary_accuracy: 0.9783\n",
      "Epoch 46/50\n",
      "1985/1985 [==============================] - 9s 4ms/step - loss: 0.0665 - binary_accuracy: 0.9784\n",
      "Epoch 47/50\n",
      "1985/1985 [==============================] - 9s 4ms/step - loss: 0.0653 - binary_accuracy: 0.9786\n",
      "Epoch 48/50\n",
      "1985/1985 [==============================] - 9s 4ms/step - loss: 0.0644 - binary_accuracy: 0.9786\n",
      "Epoch 49/50\n",
      "1985/1985 [==============================] - 9s 4ms/step - loss: 0.0650 - binary_accuracy: 0.9785\n",
      "Epoch 50/50\n",
      "1985/1985 [==============================] - 9s 4ms/step - loss: 0.0641 - binary_accuracy: 0.9788\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1dd9c80b400>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.fit(x_train, y_train, batch_size=64, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8eb1497d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3969/3969 [==============================] - 7s 2ms/step\n",
      "993/993 [==============================] - 2s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = ann.predict(x_train)\n",
    "y_pred_test = ann.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "040cc716",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_train_acc = accuracy_score(y_train, y_pred_train.round())\n",
    "ann_test_acc = accuracy_score(y_test, y_pred_test.round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1d4c3c13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9075246464455573"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1a4668d1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9197442398853489"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d2ae3da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_train_loss = hamming_loss(y_train, y_pred_train.round())\n",
    "ann_test_loss = hamming_loss(y_test, y_pred_test.round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "658e6832",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.018223958087258716 0.0224626497422491\n"
     ]
    }
   ],
   "source": [
    "print(ann_train_loss, ann_test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb94c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
